{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "from pyproj import Proj\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Constants\n",
    "RAW_SEQ_LEN = 50\n",
    "PAD = 0\n",
    "K = 5\n",
    "DISTANCE_THRESHOLD = 100\n",
    "\n",
    "# 1. Load the Data\n",
    "poi_cat_vecs_df = pd.read_parquet(\"/Users/mehul/Downloads/novateur.phase2.trial4/poi_cat_vecs.parquet\")\n",
    "poi_df = pd.read_csv(\"/Users/mehul/Downloads/novateur.phase2.trial4/poi.csv\")\n",
    "stay_poi_df = pd.read_parquet(\"/Users/mehul/Downloads/novateur.phase2.trial4/stay_poi_dfs/group=0/stay_poi.parquet\")\n",
    "\n",
    "# 2. Project Coordinates to UTM\n",
    "\n",
    "### Borrowed from TrajGPT ###\n",
    "def get_utm_zone(longitude):\n",
    "    return int((math.floor((longitude + 180) / 6) % 60) + 1)\n",
    "\n",
    "\n",
    "def project_latlon_to_xy(df):\n",
    "    # Get centroid\n",
    "    lat_c, lon_c = df[['latitude', 'longitude']].mean().values\n",
    "\n",
    "    # Initialize UTM projection\n",
    "    zone_number = get_utm_zone(lon_c)\n",
    "    project_utm = Proj(proj='utm', zone=zone_number, ellps='WGS84')\n",
    "\n",
    "    # Project centroid to UTM\n",
    "    utm_x_c, utm_y_c = project_utm(lon_c, lat_c)\n",
    "\n",
    "    utm_x, utm_y = project_utm(df['longitude'], df['latitude'])\n",
    "    x = utm_x - utm_x_c  # Unit: meter\n",
    "    y = utm_y - utm_y_c  # Unit: meter\n",
    "    return x, y\n",
    "\n",
    "### \n",
    "\n",
    "stay_poi_df['x'], stay_poi_df['y'] = project_latlon_to_xy(stay_poi_df)\n",
    "poi_df['x'], poi_df['y'] = project_latlon_to_xy(poi_df)\n",
    "\n",
    "# 3. Calculate Duration (in Hours)\n",
    "stay_poi_df['start_timestamp'] = pd.to_datetime(stay_poi_df['start_timestamp'])\n",
    "stay_poi_df['stop_timestamp'] = pd.to_datetime(stay_poi_df['stop_timestamp'])\n",
    "stay_poi_df['duration'] = (stay_poi_df['stop_timestamp'] - stay_poi_df['start_timestamp']).dt.total_seconds() / 3600\n",
    "\n",
    "# 4. Assign K nearest POIs from poi_df to each visit in stay_poi_df (create subset for POI attribution)\n",
    "poi_tree = cKDTree(poi_df[['x', 'y']].values)\n",
    "\n",
    "def find_candidate_pois(row):\n",
    "    \"\"\"Find the K nearest POIs within the distance threshold.\"\"\"\n",
    "    distances, indices = poi_tree.query([row['x'], row['y']], k=K, distance_upper_bound=DISTANCE_THRESHOLD)\n",
    "    valid_indices = indices[distances < DISTANCE_THRESHOLD]\n",
    "    if len(valid_indices) == 0:\n",
    "        return np.array([-1])  # Indicates no POIs within threshold\n",
    "    return poi_df.iloc[valid_indices]['poi_id'].values\n",
    "\n",
    "stay_poi_df['candidate_pois'] = stay_poi_df.apply(find_candidate_pois, axis=1)\n",
    "\n",
    "# 5. Map true POI to candidate index if poi_id exists in stay_poi_df\n",
    "if 'poi_id' in stay_poi_df.columns:\n",
    "    def get_true_index(row):\n",
    "        \"\"\"Map true poi_id to its index in candidate_pois.\"\"\"\n",
    "        if row['poi_id'] in row['candidate_pois']:\n",
    "            return list(row['candidate_pois']).index(row['poi_id'])\n",
    "        return -1  # True POI not in candidates\n",
    "    stay_poi_df['true_poi_index'] = stay_poi_df.apply(get_true_index, axis=1)\n",
    "\n",
    "# 6. Normalize time and compute travel time\n",
    "oldest_time = stay_poi_df['start_timestamp'].min()\n",
    "stay_poi_df['start_timestamp'] = (stay_poi_df['start_timestamp'] - oldest_time).dt.total_seconds() / (24 * 3600)  # Days\n",
    "stay_poi_df['stop_timestamp'] = (stay_poi_df['stop_timestamp'] - oldest_time).dt.total_seconds() / (24 * 3600)  # Days\n",
    "stay_poi_df['travel_time'] = stay_poi_df['start_timestamp'] - stay_poi_df['stop_timestamp'].shift(1).fillna(0)  # Days\n",
    "stay_poi_df.loc[stay_poi_df.groupby('agent').head(1).index, 'travel_time'] = 0  # Reset travel time for first visit per user\n",
    "\n",
    "# 7. Clip outliers\n",
    "max_duration = stay_poi_df['duration'].quantile(0.99)\n",
    "stay_poi_df.loc[stay_poi_df['duration'] > max_duration, 'duration'] = max_duration\n",
    "max_travel_time = stay_poi_df['travel_time'].quantile(0.99) * 24  # Convert to hours for clipping\n",
    "stay_poi_df.loc[stay_poi_df['travel_time'] * 24 > max_travel_time, 'travel_time'] = max_travel_time / 24\n",
    "\n",
    "# 8. Sort and split into sequences\n",
    "\n",
    "### Borrowed from TrajGPT ###\n",
    "def split_indices_for_next_prediction(df):\n",
    "    # Identify instances for next visit prediction\n",
    "    groupby_user = df.groupby('agent')\n",
    "\n",
    "    # Find the first index of each instance\n",
    "    # 1. First instance of each user\n",
    "    user_first_indices = groupby_user.nth(0).index\n",
    "    # 2. Instance with at least RAW_SEQ_LEN visits\n",
    "    max_num_visits_per_user = groupby_user.size().max()\n",
    "    long_enough_indices = groupby_user.nth(list(range(-max_num_visits_per_user, -RAW_SEQ_LEN))).index\n",
    "    # 3. Combine the two\n",
    "    first_indices = user_first_indices.union(long_enough_indices)\n",
    "\n",
    "    # Find the corresponding last indices\n",
    "    # 1. Last index of each user\n",
    "    each_user_last_index = groupby_user.nth(-1).set_index('agent')['id']\n",
    "    # 2. Match it with first_indices\n",
    "    user_last_indices = df.loc[first_indices]['agent'].apply(lambda user_id: each_user_last_index.loc[user_id]).values\n",
    "    # 3. Last index of each rolling window\n",
    "    rolling_window_last_indices = (first_indices + RAW_SEQ_LEN - 1).values\n",
    "    last_indices = np.minimum(rolling_window_last_indices, user_last_indices)\n",
    "\n",
    "    # Sort instances by arrival_time\n",
    "    index_df = pd.DataFrame({'first_index': first_indices, 'last_index': last_indices, 'start_timestamp': df.loc[first_indices]['start_timestamp']})\n",
    "    index_df.sort_values(by='start_timestamp', inplace=True, ignore_index=True)\n",
    "    index_df.drop(columns=['start_timestamp'], inplace=True)\n",
    "\n",
    "    # Split instances into train, validation, and test sets\n",
    "    train_size = int(0.8 * len(index_df))\n",
    "    val_size = int(0.1 * len(index_df))\n",
    "    train_indices = index_df[:train_size].reset_index(drop=True)\n",
    "    val_indices = index_df[train_size: train_size + val_size].reset_index(drop=True)\n",
    "    test_indices = index_df[train_size + val_size:].reset_index(drop=True)\n",
    "\n",
    "    return train_indices, val_indices, test_indices\n",
    "###\n",
    "\n",
    "stay_poi_df.sort_values(by=['agent', 'start_timestamp'], inplace=True)\n",
    "train_indices, val_indices, test_indices = split_indices_for_next_prediction(stay_poi_df)\n",
    "\n",
    "# 9. Custom dataset classes\n",
    "class POIPredictionDataset(Dataset):\n",
    "    def __init__(self, df, indices):\n",
    "        self.df = df\n",
    "        self.indices = indices.astype(int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        first_idx, last_idx = self.indices.iloc[idx].values\n",
    "        instance = self.df.iloc[first_idx:last_idx + 1]\n",
    "        visits = instance[['x', 'y', 'start_timestamp', 'end_timestamp', 'duration', 'travel_time']].values\n",
    "        candidate_pois = np.stack(instance['candidate_pois'].values)\n",
    "        true_poi_indices = instance['true_poi_index'].values if 'true_poi_index' in instance.columns else np.full(len(instance), -1)\n",
    "\n",
    "        seq_len = visits.shape[0]\n",
    "        if seq_len < RAW_SEQ_LEN:\n",
    "            pad_len = RAW_SEQ_LEN - seq_len\n",
    "            visits = np.pad(visits, ((pad_len, 0), (0, 0)), constant_values=PAD)\n",
    "            candidate_pois = np.pad(candidate_pois, ((pad_len, 0), (0, 0)), constant_values=-1)\n",
    "            true_poi_indices = np.pad(true_poi_indices, (pad_len, 0), constant_values=-1)\n",
    "\n",
    "        return {\n",
    "            'visits': torch.tensor(visits, dtype=torch.float),         # Visit features\n",
    "            'candidate_pois': torch.tensor(candidate_pois, dtype=torch.long),  # POI IDs of candidates\n",
    "            'true_poi_indices': torch.tensor(true_poi_indices, dtype=torch.long)  # Index of true POI in candidates\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_data = POIPredictionDataset(stay_poi_df, train_indices)\n",
    "val_data = POIPredictionDataset(stay_poi_df, val_indices)\n",
    "test_data = POIPredictionDataset(stay_poi_df, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
